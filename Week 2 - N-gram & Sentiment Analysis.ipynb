{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NLP course  -   Research Master Business Data Science**\n",
    "\n",
    "*by Meike Morren and Bas Donkers*\n",
    "\n",
    "Inspired by: \n",
    "* https://www.depends-on-the-definition.com/introduction-n-gram-language-models/ (spaCy)\n",
    "* https://www.analyticsvidhya.com/blog/2018/02/natural-language-processing-for-beginners-using-textblob/ (textblob)\n",
    "* http://rwet.decontextualize.com/book/textblob/ (textblob and wordnet)\n",
    "* https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-1-2697bb111ed4 (overview sentiment analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, we will analyze the sentences gathered in week 1. The main goal is to show you how to use python to analyze N-grams and sentiment of the reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we start, retrieve data from other notebook\n",
    "%store -r df_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "We will work with `TextBlob` which builds on `NLTK` and `pattern`, but there are many other packages available (see for instance `spaCy`). In order to analyze text, we need to break down the sentences. This way, we can easily check whether sentence parts are frequently used or not. The most simple N-gram is the unigram, where each word is analyzed separately. \n",
    "So to calculate the probability of word $w_{i}$ given the other words $w$ in a sentence, you would use this formula:\n",
    "\n",
    "\\begin{equation*}\n",
    "    {P}(w_{i} \\vert w_{i-n},...,w_{i-2},w_{i-1}) = \\frac{Count(w_{i-n}...w_{i-2}w_{i-1}w_{i})}{Count(w_{i})}\n",
    "    %{P}(w_{1}^{n}) = \\prod_{k=1}^{n} {P}(w_{k} \\vert w_{k-N-1}^{k-1})%\n",
    "\\end{equation*}\n",
    "\n",
    "However most often you would make use of bigrams (two words) or trigrams (three words). If applied to a bi-gram this equation changed to:\n",
    "\\begin{equation*}\n",
    "    {P}(w_{i} \\vert w_{i-1}) = \\frac{Count(w_{i-1}w_{i})}{Count(w_{i})}\n",
    "    %{P}(w_{i} \\vert w_{1}w_{2}...w_{i-1}) = {P}(w_{i} \\vert w_{i-1})%\n",
    "\\end{equation*}\n",
    "\n",
    "For a graphical demonstration of creating bigrams, see the picture below (from spaCy blog)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/images/Ngram-language-model-explained-with-examples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decide on unigrams, bigrams, or trigrams, you need to consider the size of the vocabulary. The space an n-gram requires is exponential, so the larger the N-gram, the more data you would need:\n",
    "\\begin{equation*}\n",
    "    n(w-2(n-1))+\\sum _{i=1}^{n-1}2i\\qquad n,w\\in {\\mathcal {N}}\n",
    "\\end{equation*}\n",
    "        \n",
    "where each word $w$ in vocabulary $N$ is considered to be part of a n-gram of size $n$. \n",
    "\n",
    "For bigrams, each word combination is created. This means that before we start to divide the data into n-grams, we need to tokenize each sentence (see week 1). Ofcourse we could easily create a function ourselves, but textblob already has many possibilities. When you create a textblob, this is already tokenized, and can be used for n-grams, pos tagging, correct spelling and many other NLP tasks. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\MP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\MP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\MP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\MP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# install packages\n",
    "#Anaconda: conda install -c conda-forge textblob\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from textblob.sentiments import PatternAnalyzer\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet \n",
    "nltk.download('wordnet')\n",
    "nltk.download('brown')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TextBlob(df_sent.sentence.loc[1])) # 80 characters\n",
    "len(TextBlob(df_sent.sentence.loc[1]).words) # 15 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Really', 'great']),\n",
       " WordList(['great', 'Dutch']),\n",
       " WordList(['Dutch', 'food']),\n",
       " WordList(['food', 'felt']),\n",
       " WordList(['felt', 'like']),\n",
       " WordList(['like', 'I']),\n",
       " WordList(['I', 'was']),\n",
       " WordList(['was', 'eating']),\n",
       " WordList(['eating', 'at']),\n",
       " WordList(['at', 'my']),\n",
       " WordList(['my', 'Dutch']),\n",
       " WordList(['Dutch', 'grandmother']),\n",
       " WordList(['grandmother', 's']),\n",
       " WordList(['s', 'house'])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create n-grams\n",
    "TextBlob(df_sent.sentence.loc[1]).ngrams(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing\n",
    "N-grams can be an efficient way to feed text to a classifier. However, more information can be abstracted from text. First, each word has a certain meaning in a sentence. This can be analyzed using tags. Textblob has a parser that analyzes the structure of the sentence, and assigns a tag to each word. Note that these are not always correct! Try to guess the pos tag of *trigger* in the following two sentences:\n",
    "\n",
    "* Corona might trigger a financial crisis\n",
    "* Corona might be a trigger for a financial crisis\n",
    "\n",
    "Clearly in the first sentence *trigger* is a verb while in the second it is a noun. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Corona', 'NNP'),\n",
       " ('might', 'MD'),\n",
       " ('trigger', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('financial', 'JJ'),\n",
       " ('crisis', 'NN')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob('Corona might be a trigger for a financial crisis').tags\n",
    "TextBlob('Corona might trigger a financial crisis').tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('They', 'PRP'),\n",
       " ('only', 'RB'),\n",
       " ('sell', 'VBP'),\n",
       " ('one', 'CD'),\n",
       " ('overpriced', 'VBD'),\n",
       " ('OVERLY', 'NNP'),\n",
       " ('SWEET', 'NNP'),\n",
       " ('cookie', 'NN')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find tags\n",
    "TextBlob(df_sent.sentence.loc[1]).tags\n",
    "TextBlob(df_sent.sentence.loc[120]).tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Capitals matter!** TextBlob analyzes the text, and uses capitals as markers for certain POS taggers. If the word starts with a capital, TextBlob will give it a NN (noun) or NNP (proper nouns (NNP) tag. NNP names specific people, places, things, or ideas.  Since they these nouns are naming specific things, they always begin with a capital letter. If all letters are capitals (see second example above), then TextBlob will also regard these words as proper nouns.\n",
    "\n",
    "In order to circumvent these tags, you can use only lower case. Although it creates better tags for words that identify countries (such as Dutch food), it might also create problems for names of specific people, things, ideas and places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the tags, you can decipher phrases as we discussed in the slides. This means that a set of words are identified as a phrase, a set of words that are connected. The most common one is noun phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['morning olav', 'sweet way'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change olav to capitals (or first letter only) to see what happens\n",
    "TextBlob('this morning olav was playing with lego in a very sweet way').noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['really', 'dutch', 'dutch', 'grandmother s house'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find nouns\n",
    "TextBlob(df_sent.sentence.loc[1]).noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['great dutch food', 'dutch grandmother s house'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(df_sent.sentence.loc[1].lower()).noun_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, computers are not very good at reading human language, and if parsing already poses challenges, the dissection of noun phrases even more! You can play around with many different parsers, but overall you will find that the results are not perfect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REGEX\n",
    "Now lets move on to the corpus that we will use to create the term frequency document matrix. As symbols and words that have only one character are very unlikely to be interesting, usually these are removed from the corpus.\n",
    "\n",
    "Using the `re` function, you can remove (`re.match`), replace (`re.sub`) or find (`re.findall`) certain symbols, or set of characters in a string. Any type of character or symbol can be searched for. If you want to search for a character or word in a certain position in a string you can use * to mask the rest of the string. So if you would like to find all sentences that start with \"The\". If you're interested in position, you need to start the pattern with `^` and if you're interested in how the string ends you need to close the pattern with `$`. If you are interested in all possible number (& combinations) you can use `[0-9]`. There are endless possibilities with `re`. Please find a comprehensible overview here: *https://www.w3schools.com/python/python_regex.asp.\n",
    "\n",
    "* `findall()` will return all occurrences that match a pattern\n",
    "* `search()` module will only return the first occurrence that matches the pattern\n",
    "* `match()` checks for a match only at the beginning of the string\n",
    "* `sub()` matched occurrences are replaced with the content of replace variable\n",
    "* `\\w` will match alphanumeric characters and underscores\n",
    "* `[^\\w]` will match anything that's not alphanumeric or underscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' That said  we had a great experience-- the food here is great',\n",
       " ' It s great',\n",
       " ' Yup  it was pretty great',\n",
       " ' We thought the prices & quality of the steaks and food were great',\n",
       " 'Interesting spot in utrecht  The staff are friendly and the food is great',\n",
       " 'Steak was great',\n",
       " '  Service was great',\n",
       " 'Blossom is simply great',\n",
       " 'Kibbeling (fried fish  was great',\n",
       " ' The staff is very friendly and the service was great',\n",
       " '  \\nHad the Palak Paneer  which were great',\n",
       " 'I ve come here three times before and everything was great',\n",
       " ' The coffee guide on the walls are a great',\n",
       " ' The service was very good and the food was great',\n",
       " '\\nExcellent value for money and a great',\n",
       " 'All the food was great',\n",
       " ' The food  the drinks  the service was great',\n",
       " ' The rice bowls are great',\n",
       " ' Atmosphere was also great',\n",
       " ' The coffee and service is always great',\n",
       " 'This place was great',\n",
       " 'You must go there  the atmosphere was amazing and pizza really great',\n",
       " ' The staff is very friendly  very accommodating  food is great',\n",
       " ' The beer selection was great',\n",
       " ' The ambience and customer service was great',\n",
       " ' Lunch was great',\n",
       " '\\n Quite simply it s great',\n",
       " ' From big band to jazz  the atmosphere is always great',\n",
       " '\\n The decor of the bar and restaurant is great',\n",
       " ' My husband and I each ordered crepe and smoothie  and they were all great',\n",
       " '  \\nThe menu is expansive with great',\n",
       " '  prices are very reasonable and the service was great',\n",
       " 'Went here with the family for a quick lunch and have nothing to complain about  food was great  service was great and the atmosphere was great']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all sentences end with great\n",
    "[s for s in df_sent.sentence if re.search(\".*great$\",s)] # asterisk means everything else that precedes 'great'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I recommend seeing movie  Loving Vincent go familiarize story paintings'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=df_sent.sentence_cleaned.loc[19]\n",
    "re.findall('^[a-zA-Z]{1}$',s) # find sentences with only one letter, capital or not\n",
    "re.findall(r\"\\bhttps:\\//[a-z0-9.]*\", s) # find https links\n",
    "\n",
    "re.search('[\\n]',s) # find EOL markers\n",
    "re.search('[\\n]+\\w',s) # EOL marker followed by letter\n",
    "re.search('[*]?',s) # find star [] needed to find literally stars\n",
    "\n",
    "re.sub('\\s', '', s) # remove spaces\n",
    "re.sub('[a-z]', '', s.lower()) # remove letters, first convert to lower case\n",
    "re.sub(r'\"','',s) # r makes sure to also find \"\n",
    "re.sub(r'[^\\w]',' ',s) # replace everything but characters with spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all symbols except characters\n",
    "df_sent.sentence_cleaned = [re.sub(r'[^\\w]',' ',s) for s in df_sent.sentence_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all sentences that only contain one character\n",
    "df_sent.sentence_cleaned = [re.sub('^[a-zA-Z]{1}$','',s) for s in df_sent.sentence_cleaned]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "Next, we create a matrix where each column represents a word and each row corresponds to the text documents (in our case sentences from text documents). This matrix allows us to extract special features from the text. Be aware that this function also deletes the spaces, and the single letters that are left over by the cleaning we have done above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visiting</th>\n",
       "      <th>wait</th>\n",
       "      <th>waiter</th>\n",
       "      <th>waiters</th>\n",
       "      <th>waitlist</th>\n",
       "      <th>walk</th>\n",
       "      <th>walked</th>\n",
       "      <th>warm</th>\n",
       "      <th>waters</th>\n",
       "      <th>way</th>\n",
       "      <th>we</th>\n",
       "      <th>weekday</th>\n",
       "      <th>well</th>\n",
       "      <th>winkel</th>\n",
       "      <th>world</th>\n",
       "      <th>worst</th>\n",
       "      <th>worth</th>\n",
       "      <th>wow</th>\n",
       "      <th>wrong</th>\n",
       "      <th>x2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    visiting  wait  waiter  waiters  waitlist  walk  walked  warm  waters  \\\n",
       "0          0     0       0        0         0     0       0     0       0   \n",
       "1          0     0       0        0         0     0       0     0       0   \n",
       "2          0     0       0        0         0     0       0     0       0   \n",
       "3          0     0       0        0         0     0       0     0       0   \n",
       "4          0     0       0        0         0     0       0     0       0   \n",
       "..       ...   ...     ...      ...       ...   ...     ...   ...     ...   \n",
       "95         0     0       0        0         0     1       0     0       0   \n",
       "96         0     0       0        0         0     0       0     0       0   \n",
       "97         0     0       0        0         0     0       0     0       0   \n",
       "98         0     0       0        0         0     0       0     0       0   \n",
       "99         0     0       0        0         0     0       0     0       0   \n",
       "\n",
       "    way  we  weekday  well  winkel  world  worst  worth  wow  wrong  x2  \n",
       "0     0   0        0     0       0      0      0      0    1      0   0  \n",
       "1     0   0        0     0       0      0      0      0    0      0   0  \n",
       "2     0   0        0     0       0      0      0      0    0      0   0  \n",
       "3     0   0        0     0       0      0      0      0    0      0   0  \n",
       "4     0   0        0     0       0      0      1      0    0      0   0  \n",
       "..  ...  ..      ...   ...     ...    ...    ...    ...  ...    ...  ..  \n",
       "95    0   0        0     0       0      0      0      0    0      0   0  \n",
       "96    0   0        0     0       0      0      0      0    0      0   0  \n",
       "97    0   0        0     0       0      0      0      0    0      0   0  \n",
       "98    0   0        0     0       0      0      0      0    0      0   0  \n",
       "99    0   0        0     0       0      0      0      0    0      0   0  \n",
       "\n",
       "[100 rows x 20 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = df_sent.sentence_cleaned[0:100]\n",
    "\n",
    "BoWVectorizer = CountVectorizer()\n",
    "BoW_df = pd.DataFrame(BoWVectorizer.fit_transform(corpus).todense())\n",
    "BoW_df.columns=sorted(BoWVectorizer.vocabulary_)\n",
    "BoW_df.iloc[:,300:320] # see word 'wow' in first line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main disadvantage of BoW approach are the frequent words that contain little meaning. To avoid this, you could remove the stopwords (see week 1). Note that the count vectorizer cleaned the texts from capitals. See below for the 10 most frequent words in the first 100 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food</th>\n",
       "      <th>friendly</th>\n",
       "      <th>good</th>\n",
       "      <th>great</th>\n",
       "      <th>nice</th>\n",
       "      <th>one</th>\n",
       "      <th>place</th>\n",
       "      <th>restaurant</th>\n",
       "      <th>service</th>\n",
       "      <th>staff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3018</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3019</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3020</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3021</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3022</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3023 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      food  friendly  good  great  nice  one  place  restaurant  service  \\\n",
       "0        0         0     0      0     0    0      1           0        0   \n",
       "1        1         0     0      1     0    0      0           0        0   \n",
       "2        0         0     0      0     0    0      0           0        0   \n",
       "3        0         1     0      0     0    0      0           0        1   \n",
       "4        1         0     0      0     0    0      0           1        0   \n",
       "...    ...       ...   ...    ...   ...  ...    ...         ...      ...   \n",
       "3018     0         0     0      0     0    0      0           0        0   \n",
       "3019     0         0     0      0     0    0      0           0        0   \n",
       "3020     0         0     0      0     0    0      0           0        0   \n",
       "3021     0         0     0      0     0    0      0           0        0   \n",
       "3022     0         0     0      0     0    0      0           0        0   \n",
       "\n",
       "      staff  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "...     ...  \n",
       "3018      0  \n",
       "3019      0  \n",
       "3020      0  \n",
       "3021      0  \n",
       "3022      0  \n",
       "\n",
       "[3023 rows x 10 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = df_sent.sentence_cleaned\n",
    "BoW = CountVectorizer(max_features=10)\n",
    "BoW_df = pd.DataFrame(BoW.fit_transform(corpus).todense())\n",
    "BoW_df.columns=sorted(BoW.vocabulary_)\n",
    "BoW_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3953"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter \n",
    "# remove words that are single characters\n",
    "tokens_corpus = [token for s in df_sent.sentence_cleaned for token in word_tokenize(s) if len(token) > 1] \n",
    "len(Counter(tokens_corpus)) # 3896 unique words in vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "Another way to deal with frequent words is to use the Term Frequency (TF) per document. This is summarized in the term frequency - document matrix where each frequency relates to term *t* in document *d*. How often term $t$ occurs in document $d$ and all documents of $D$, you can use the inverse fraction of documents:\n",
    "        \\begin{equation*}\n",
    "            idf(t,D) = log \\frac{N}{1 + \\lvert \\left\\{d \\in D : t \\in D \\right\\}\\rvert}\n",
    "        \\end{equation*}\n",
    "To prevent bias towards larger documents:\n",
    "\\begin{equation*}\n",
    "    tf(t,d) = 0.5 + 0.5 \\cdot \\frac{f_{t,d}}{max \\left\\{f_{t',d} : t^\\prime \\in d  \\right\\}}\n",
    "\\end{equation*}\n",
    "Hence, the `TfidfVectorizer` does two things: transform the data to a term - frequency matrix, and calculate the frequency of terms across documents. It uses a log transformation (`fit_transform()`), and adds one (laplace smoothing) to make sure that words with an idf score of zero are not surpressed entirely (fit function actived when running `TfidfVectorizer()`). For more information see: https://www.freecodecamp.org/news/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>1150</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>15</th>\n",
       "      <th>150</th>\n",
       "      <th>1587</th>\n",
       "      <th>...</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yup</th>\n",
       "      <th>yuuuup</th>\n",
       "      <th>zeezicht</th>\n",
       "      <th>zero</th>\n",
       "      <th>zot</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zwaan</th>\n",
       "      <th>über</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3018</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3019</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3020</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3021</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3022</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3023 rows × 3467 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       00   10  100   11  1150   12   13   15  150  1587  ...  yum  yummy  \\\n",
       "0     0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...  0.0    0.0   \n",
       "1     0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...  0.0    0.0   \n",
       "2     0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...  0.0    0.0   \n",
       "3     0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...  0.0    0.0   \n",
       "4     0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...  0.0    0.0   \n",
       "...   ...  ...  ...  ...   ...  ...  ...  ...  ...   ...  ...  ...    ...   \n",
       "3018  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...  0.0    0.0   \n",
       "3019  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...  0.0    0.0   \n",
       "3020  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...  0.0    0.0   \n",
       "3021  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...  0.0    0.0   \n",
       "3022  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0   0.0  ...  0.0    0.0   \n",
       "\n",
       "      yup  yuuuup  zeezicht  zero  zot  zucchini  zwaan  über  \n",
       "0     0.0     0.0       0.0   0.0  0.0       0.0    0.0   0.0  \n",
       "1     0.0     0.0       0.0   0.0  0.0       0.0    0.0   0.0  \n",
       "2     0.0     0.0       0.0   0.0  0.0       0.0    0.0   0.0  \n",
       "3     0.0     0.0       0.0   0.0  0.0       0.0    0.0   0.0  \n",
       "4     0.0     0.0       0.0   0.0  0.0       0.0    0.0   0.0  \n",
       "...   ...     ...       ...   ...  ...       ...    ...   ...  \n",
       "3018  0.0     0.0       0.0   0.0  0.0       0.0    0.0   0.0  \n",
       "3019  0.0     0.0       0.0   0.0  0.0       0.0    0.0   0.0  \n",
       "3020  0.0     0.0       0.0   0.0  0.0       0.0    0.0   0.0  \n",
       "3021  0.0     0.0       0.0   0.0  0.0       0.0    0.0   0.0  \n",
       "3022  0.0     0.0       0.0   0.0  0.0       0.0    0.0   0.0  \n",
       "\n",
       "[3023 rows x 3467 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_df = pd.DataFrame(tfidf.fit_transform(corpus).todense())\n",
    "tfidf_df.columns=sorted(tfidf.vocabulary_)\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zeros mean that these words are present in multiple reviews, while the numbers different from zero represent words that only appear in that document (i.e. sentence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization, Stemming & WordNet\n",
    "Many words spring from the same stem, i.e. the part of the word to which affixes can be attached. Stems have lexical meaning. For example, the English word *friendships* contains the stem *friend*. Words having the same stem often share meaning. Stems do not need to be actual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'realli great dutch food felt like i wa eat at my dutch grandmoth s hous'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = df_sent.sentence[1].lower()\n",
    "ps_stemmer = PorterStemmer()\n",
    "' '.join([ps_stemmer.stem(word) for word in word_tokenize(sentence)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes stemming is limited as the algorithms just cut off the end or beginning of a word (based on a list of common prefixes and suffixes that can be found). Lemmatization takes into consideration the morpholical analysis of the words. It looks through detailed dictionaries to link the form back to its lemma. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'really great dutch food felt like i wa eating at my dutch grandmother s house'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(sentence)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of lemmatized words, is that you can easily find other words that are related. These are called synsets and are stored in WordNet, an enormous endeavour by many researchers to document the available words in English language and how they are related to one another. Synsets describe words that are similar in meaning. To learn more about synsets, you can use wordnet which is integrated with textblob. This might be useful, if you would like to take into account the multiple meanings of the word 'bank' for instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sloping land (especially the slope beside a body of water) , example: ['they pulled the canoe up on the bank', 'he sat on the bank of the river and watched the currents']\n",
      "1 a financial institution that accepts deposits and channels the money into lending activities , example: ['he cashed a check at the bank', 'that bank holds the mortgage on my home']\n",
      "2 a long ridge or pile , example: ['a huge bank of earth']\n",
      "3 an arrangement of similar objects in a row or in tiers , example: ['he operated a bank of switches']\n",
      "4 a supply or stock held in reserve for future use (especially in emergencies) , example: []\n",
      "5 the funds held by a gambling house or the dealer in some gambling games , example: ['he tried to break the bank at Monte Carlo']\n",
      "6 a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force , example: []\n",
      "7 a container (usually with a slot in the top) for keeping money at home , example: ['the coin bank was empty']\n",
      "8 a building in which the business of banking transacted , example: ['the bank is on the corner of Nassau and Witherspoon']\n",
      "9 a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning) , example: ['the plane went into a steep bank']\n",
      "10 tip laterally , example: ['the pilot had to bank the aircraft']\n",
      "11 enclose with a bank , example: ['bank roads']\n",
      "12 do business with a bank or keep an account at a bank , example: ['Where do you bank in this town?']\n",
      "13 act as the banker in a game or in gambling , example: []\n",
      "14 be in the banking business , example: []\n",
      "15 put into a bank account , example: ['She deposits her paycheck every month']\n",
      "16 cover with ashes so to control the rate of burning , example: ['bank a fire']\n",
      "17 have confidence or faith in , example: ['We can trust in God', 'Rely on your friends', 'bank on your good education', \"I swear by my grandmother's recipes\"]\n"
     ]
    }
   ],
   "source": [
    "syn=wordnet.synsets('bank')\n",
    "for k in range(0,len(syn)):\n",
    "    print(k, syn[k].definition(),', example:', syn[k].examples()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid separately analyzing words with similar meaning, you could stem or lemmatize all words in your corpus. Ofcourse this deletes information, so if you think there is valuable information of whether a word is a verb or noun, you should not stem or lemmatize. In most cases, if you have enough data, it might be better to analyze the words as they are. However, in small datasets, you need to limit the vocabulary size, for which lemmatization and stemming can be very useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment\n",
    "To analyze sentiment, you can use rule based methods (such as textblob, or vader) or we can use feature-based mehods such as the Naive Bayes classifier, Support Vector Machines classifier, or the pattern classifier. The advantage of rule-based methods is that they are simple and easy to follow. The downside is that the rules may be too simplistic to pick up the subtitles of human langauge (think of comments that both include positive and negative sentiments). The default in TextBlob is the Pattern Analyzer, which is based on the pattern package. Note that if the polarity is >0, it is considered positive, <0 -is considered negative and ==0 is considered neutral. The Naive Bayes classifier in TextBlob is build on the NLTK package and is trained on IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Really great Dutch food  felt like I was eating at my Dutch grandmother s house\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.8, subjectivity=0.75)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_sent.sentence.loc[1])\n",
    "TextBlob(df_sent.sentence.loc[1]).sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5833333333333334"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# negative sentiment with textblob, naive bayes\n",
    "TextBlob(df_sent.sentence.loc[1], analyzer=NaiveBayesAnalyzer()).sentiment\n",
    "\n",
    "# polarity and subjectivity with textblob, pattern analyzer\n",
    "TextBlob(df_sent.sentence.loc[1], analyzer=PatternAnalyzer()).sentiment\n",
    "\n",
    "# negative example\n",
    "TextBlob(df_sent.sentence.loc[7], analyzer=NaiveBayesAnalyzer()).sentiment\n",
    "TextBlob(df_sent.sentence.loc[7], analyzer=PatternAnalyzer()).sentiment[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow  this place is just so real and authentic {'neg': 0.0, 'neu': 0.678, 'pos': 0.322, 'compound': 0.5859}\n",
      " Really great Dutch food  felt like I was eating at my Dutch grandmother s house {'neg': 0.0, 'neu': 0.615, 'pos': 0.385, 'compound': 0.7841}\n",
      " As K and I were here on a {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "Should be 1 star but friendly service 2 stars {'neg': 0.0, 'neu': 0.583, 'pos': 0.417, 'compound': 0.6486}\n",
      "  \n",
      "Food here is atrocious  the worst restaurant I ve had in Amsterdam by a LONG SHOT {'neg': 0.24, 'neu': 0.76, 'pos': 0.0, 'compound': -0.6249}\n",
      " Any menu with multiple {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "Excellent service and food {'neg': 0.0, 'neu': 0.448, 'pos': 0.552, 'compound': 0.5719}\n",
      " You can t really go wrong with Dutch home cooking {'neg': 0.297, 'neu': 0.703, 'pos': 0.0, 'compound': -0.5233}\n",
      "  \n",
      "The pea soup was delightful {'neg': 0.0, 'neu': 0.513, 'pos': 0.487, 'compound': 0.5859}\n",
      " I got the meatball with three different types {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in some cases vader is better  at finding subtle meanings of sentences\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "[print(s, vader.polarity_scores(s)) for s in df_sent.sentence[0:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>compound</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>polarity</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.586285</td>\n",
       "      <td>0.683865</td>\n",
       "      <td>0.195327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subjectivity</th>\n",
       "      <td>0.586285</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.501763</td>\n",
       "      <td>0.057582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compound</th>\n",
       "      <td>0.683865</td>\n",
       "      <td>0.501763</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.216375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rating</th>\n",
       "      <td>0.195327</td>\n",
       "      <td>0.057582</td>\n",
       "      <td>0.216375</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              polarity  subjectivity  compound    rating\n",
       "polarity      1.000000      0.586285  0.683865  0.195327\n",
       "subjectivity  0.586285      1.000000  0.501763  0.057582\n",
       "compound      0.683865      0.501763  1.000000  0.216375\n",
       "rating        0.195327      0.057582  0.216375  1.000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add these findings to the dataset and relate them to rating.\n",
    "df_sent['polarity'] = [TextBlob(s, analyzer=PatternAnalyzer()).sentiment[0] for s in df_sent.sentence]\n",
    "df_sent['subjectivity'] = [TextBlob(s, analyzer=PatternAnalyzer()).sentiment[1] for s in df_sent.sentence]\n",
    "df_sent['compound'] = [vader.polarity_scores(s)['compound'] for s in df_sent.sentence]\n",
    "df_sent['rating'] = df_sent.rating.astype('int64')\n",
    "df_sent[['polarity','subjectivity','compound','rating']].corr(method='pearson') # .195"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've got the sentiment per sentence, you could create a sentiment per review (document). Taking the mean across sentences within a review could be useful if reviews only contain negative or only positive sentences. However, if the review is mixed you will wrongly conclude that the review is neutral. To take this into account, you could create a measure based on variance. The more variance in sentiment a review contains, the lower the correlation between sentiment and rating. This way, you can correct for the reviews with mixed messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity_mean</th>\n",
       "      <th>subjectivity_mean</th>\n",
       "      <th>polarity_var</th>\n",
       "      <th>subjectivity_var</th>\n",
       "      <th>compound_mean</th>\n",
       "      <th>compound_var</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>polarity_mean</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.588300</td>\n",
       "      <td>0.202276</td>\n",
       "      <td>-0.031867</td>\n",
       "      <td>0.673767</td>\n",
       "      <td>-0.026095</td>\n",
       "      <td>0.286109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subjectivity_mean</th>\n",
       "      <td>0.588300</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.235507</td>\n",
       "      <td>0.013681</td>\n",
       "      <td>0.512301</td>\n",
       "      <td>0.024136</td>\n",
       "      <td>0.089925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polarity_var</th>\n",
       "      <td>0.202276</td>\n",
       "      <td>0.235507</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.347621</td>\n",
       "      <td>0.071258</td>\n",
       "      <td>0.310441</td>\n",
       "      <td>0.017748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subjectivity_var</th>\n",
       "      <td>-0.031867</td>\n",
       "      <td>0.013681</td>\n",
       "      <td>0.347621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.082701</td>\n",
       "      <td>0.205515</td>\n",
       "      <td>0.019019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compound_mean</th>\n",
       "      <td>0.673767</td>\n",
       "      <td>0.512301</td>\n",
       "      <td>0.071258</td>\n",
       "      <td>-0.082701</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.065602</td>\n",
       "      <td>0.309582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compound_var</th>\n",
       "      <td>-0.026095</td>\n",
       "      <td>0.024136</td>\n",
       "      <td>0.310441</td>\n",
       "      <td>0.205515</td>\n",
       "      <td>-0.065602</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.017976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rating</th>\n",
       "      <td>0.286109</td>\n",
       "      <td>0.089925</td>\n",
       "      <td>0.017748</td>\n",
       "      <td>0.019019</td>\n",
       "      <td>0.309582</td>\n",
       "      <td>0.017976</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   polarity_mean  subjectivity_mean  polarity_var  \\\n",
       "polarity_mean           1.000000           0.588300      0.202276   \n",
       "subjectivity_mean       0.588300           1.000000      0.235507   \n",
       "polarity_var            0.202276           0.235507      1.000000   \n",
       "subjectivity_var       -0.031867           0.013681      0.347621   \n",
       "compound_mean           0.673767           0.512301      0.071258   \n",
       "compound_var           -0.026095           0.024136      0.310441   \n",
       "rating                  0.286109           0.089925      0.017748   \n",
       "\n",
       "                   subjectivity_var  compound_mean  compound_var    rating  \n",
       "polarity_mean             -0.031867       0.673767     -0.026095  0.286109  \n",
       "subjectivity_mean          0.013681       0.512301      0.024136  0.089925  \n",
       "polarity_var               0.347621       0.071258      0.310441  0.017748  \n",
       "subjectivity_var           1.000000      -0.082701      0.205515  0.019019  \n",
       "compound_mean             -0.082701       1.000000     -0.065602  0.309582  \n",
       "compound_var               0.205515      -0.065602      1.000000  0.017976  \n",
       "rating                     0.019019       0.309582      0.017976  1.000000  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rev = pd.DataFrame()\n",
    "df_rev['rating'] = df_sent.groupby(['reviewid']).rating.mean()\n",
    "df_rev['length'] = df_sent.groupby(['reviewid']).sentence.count()\n",
    "df_rev['polarity_mean'] = df_sent.groupby(['reviewid']).polarity.mean()\n",
    "df_rev['polarity_var'] = df_sent.groupby(['reviewid']).polarity.var()\n",
    "df_rev['subjectivity_mean'] = df_sent.groupby(['reviewid']).subjectivity.mean()\n",
    "df_rev['subjectivity_var'] = df_sent.groupby(['reviewid']).subjectivity.var()\n",
    "df_rev['compound_mean'] = df_sent.groupby(['reviewid']).compound.mean()\n",
    "df_rev['compound_var'] = df_sent.groupby(['reviewid']).compound.var()\n",
    "df_rev[['polarity_mean','subjectivity_mean','polarity_var','subjectivity_var','compound_mean','compound_var','rating']].corr(method='pearson') # .195"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the rating is more highly related to the polarity averaged across the review, than per sentence. This makes sense, as the rating is a review-level variable. However, variance does not seem to play a role in rating, which may suggest that most reviews either contain negative or positive sentences. Similarly, the subjectivity score does not relate with rating, which makes sense, because both negative and positive statements may be very subjective. Compound score calculated by Vader has the highest overlap with rating, which is also found here: https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-1-2697bb111ed4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "To see how successful the prediction is, you can use a confusion matrix. This matrix matches the predicted labels with the true labels. First, categorize the scores calculated by TextBlob and Vader to match with the rating (which is the true label). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev['pred_blob'] = pd.cut(df_rev['polarity_mean'], bins=5, labels=[1, 2, 3, 4, 5])\n",
    "df_rev['pred_vader'] = pd.cut(df_rev['compound_mean'], bins=5, labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x15c3117ca88>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "\n",
    "cf_matrix = pd.DataFrame(confusion_matrix(df_rev['rating'],df_rev['pred_vader']), \n",
    "                         index = [i for i in range(1,6)], columns = [i for i in range(1,6)])\n",
    "sn.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.1%', cmap='Reds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have used rule-based methods to predict rating. However, another class of methods are feature-based. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusted from https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-1-2697bb111ed4\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "class LogisticRegressionSentiment():\n",
    "    \"\"\"Predict fine-grained sentiment scores using a sklearn Logistic Regression pipeline.\"\"\"\n",
    "    def __init__(self, model_file: str=None) -> None:\n",
    "        super().__init__()\n",
    "        from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        self.pipeline = Pipeline(\n",
    "            [\n",
    "                ('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(solver='liblinear', multi_class='auto')),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def predict(self, train_file: str, test_file: str, lower_case: bool=False) -> pd.DataFrame:\n",
    "        \"Train model using sklearn pipeline\"\n",
    "        train_df = train_file[['text','truth']]\n",
    "        learner = self.pipeline.fit(train_df['text'].str.lower(), train_df['truth'].astype(int))\n",
    "        # Predict class labels using the learner and output DataFrame\n",
    "        test_df = test_file[['text','truth']]\n",
    "        test_df['pred'] = learner.predict(test_df['text'].str.lower())\n",
    "        return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MP\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>truth</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6514</th>\n",
       "      <td>Excellent for 'gezellig  lunchen - delicious d...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3773</th>\n",
       "      <td>Take time and walk through every greenhouse</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>Our airbnb host said this is the best pizza in...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>This tucked away restaurant in Amsterdam s cit...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>I was hungry</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>The Eggs Benedict was delicious - it was on ...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195</th>\n",
       "      <td>The burger was yummy (15€ \\n Ambiance  nice  ...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>The food is creative and excellent</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>it s so good</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>The Mauritshuis is one of The Netherlands  pre...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>907 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  truth  pred\n",
       "6514  Excellent for 'gezellig  lunchen - delicious d...      5     5\n",
       "3773        Take time and walk through every greenhouse      5     5\n",
       "709   Our airbnb host said this is the best pizza in...      5     5\n",
       "449   This tucked away restaurant in Amsterdam s cit...      5     5\n",
       "103                                        I was hungry      5     5\n",
       "...                                                 ...    ...   ...\n",
       "1825    The Eggs Benedict was delicious - it was on ...      5     5\n",
       "2195   The burger was yummy (15€ \\n Ambiance  nice  ...      5     5\n",
       "1296                 The food is creative and excellent      5     5\n",
       "5570                                       it s so good      5     5\n",
       "1746  The Mauritshuis is one of The Netherlands  pre...      5     5\n",
       "\n",
       "[907 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 70% is used to train, 30% of the data is used to test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df_sent['sentence']\n",
    "y = df_sent['rating']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "train_file = pd.DataFrame({'truth': y_train, 'text': X_train})\n",
    "test_file = pd.DataFrame({'truth': y_test, 'text': X_test})\n",
    "logsentiment = LogisticRegressionSentiment()\n",
    "logsentiment.predict(train_file, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MP\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({5: 825, 4: 82})\n",
      "Counter({5: 538, 4: 264, 3: 66, 2: 21, 1: 18})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MP\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "print(Counter(logsentiment.predict(train_file, test_file).pred)) # 82 are predicted as 4 stars\n",
    "print(Counter(logsentiment.predict(train_file, test_file).truth)) # while in reality 264 have 4 stars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, a logistic regression is inclined to assign all instances to a majority class (as this dataset is very unbalanced - many reviews with 5 stars). We have used a simple tf-idf approach to represent the sentences. However, in a similar way you could use Naive Bayes or Support Vector Machines to predict the ratings and use laplace smoothing as an input to the model. See below for an example on Laplace smoothing (by Laurence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MP\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\MP\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider the three probability vectors: \n",
      " The data distribution; \n",
      " [0.026420079260237782, 0.028401585204755615, 0.07661822985468957, 0.28731836195508587, 0.5812417437252312] \n",
      " validation actual labels distribution; \n",
      " [0.020833333333333332, 0.02412280701754386, 0.07346491228070176, 0.2905701754385965, 0.5910087719298246] \n",
      " validation predicted labels distribution; \n",
      " [0.0010964912280701754, 0.0010964912280701754, 0.0010964912280701754, 0.09100877192982457, 0.9057017543859649].\n",
      "Which could be visualised as: \n",
      "   Data distribution  Truth distribution  Prediction distribution\n",
      "1           0.026420            0.020833                 0.001096\n",
      "2           0.028402            0.024123                 0.001096\n",
      "3           0.076618            0.073465                 0.001096\n",
      "4           0.287318            0.290570                 0.091009\n",
      "5           0.581242            0.591009                 0.905702\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# aquire vector where every column represents smoothed probability given the truth\n",
    "# aquire vector where every column represents smoothed probability given the predictions\n",
    "c_pred = Counter(logsentiment.predict(train_file, test_file).pred)\n",
    "c_truth = Counter(logsentiment.predict(train_file, test_file).truth)\n",
    "c_data = Counter(df_sent['rating'])\n",
    "\n",
    "pred_vector = [(c_pred[x] + 1)/(sum(c_pred.values()) + 5) for x in range(1,6)]\n",
    "truth_vector = [(c_truth[x] + 1)/(sum(c_truth.values()) + 5) for x in range(1,6)]\n",
    "data_vector = [(c_data[x] + 1)/(sum(c_data.values()) + 5) for x in range(1,6)]\n",
    "\n",
    "print(f\"Consider the three probability vectors: \\n The data distribution; \\n {data_vector} \\n validation actual labels distribution; \\n {truth_vector} \\n validation predicted labels distribution; \\n {pred_vector}.\")\n",
    "print(\"Which could be visualised as: \")\n",
    "\n",
    "df = pd.DataFrame(data = [data_vector, truth_vector, pred_vector], columns = ['1', '2', '3', '4', '5'], \n",
    "                  index = [\"Data distribution\", \"Truth distribution\", \"Prediction distribution\"]).T\n",
    "print(df)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(16,16))\n",
    "df[\"Data distribution\"].plot(ax=axes[0], kind='bar', grid=True, title = \"Data distribution\")\n",
    "df[\"Truth distribution\"].plot(ax=axes[1], kind='bar', grid=True, title = \"Truth distribution\",)\n",
    "df[\"Prediction distribution\"].plot(ax=axes[2], kind='bar', grid=True, title = \"Prediction distribution\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the data distribution and the truth distribution are very similair which makes sense since the truth distribution essentially is a sub set drawn from the real data. However, the prediction distribution is skewed towards the right and essentially has no data points for the first three categories, the only reason they have some value is due to the smoothing to ensure mathematical stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy\n",
    "To evaluate your model cross entropy is able to compare various probability distributions. The cross-entropy quantifies how similar two distributions are. The entropy can also be calculated of one distribution, in that case, entropy quantifies the uncertainty involved when encountering a random variable. \n",
    "\\begin{equation*}\n",
    "     H({\\tilde{p}},q)=-\\sum_{i=1}^{N}{\\tilde {p}}(x)\\log _{2}q(x_{i}) \n",
    "\\end{equation*}\n",
    "Cross-entropy builds upon the idea of entropy from information theory and calculates the number of bits required to represent or transmit an average event from one distribution compared to another distribution. It is simply the ratio of the entropy of two distributions: $H(P/Q)$ where $P$ might be the observed distribution and $Q$ the predicted distribution. The closer $H(P)$ is to $H(P/Q)$, the better $H(Q)$ is an approximation of $H(P)$.\n",
    "\n",
    "For a derivation of the cross-entropy in logistic regression, please see: https://peterroelants.github.io/posts/cross-entropy-logistic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy: 1.540 bits\n",
      "entropy: 1.489 bits\n",
      "cross entropy: 0.967 bits\n"
     ]
    }
   ],
   "source": [
    "# calculate the entropy for the estimated probabilities\n",
    "from math import log2\n",
    "entropy_data = - sum([data_vector[i] * log2(data_vector[i]) for i in range(len(data_vector))])\n",
    "print('entropy: %.3f bits' % entropy_data)\n",
    "\n",
    "# calculate entropy for target probabilities\n",
    "entropy_target = - sum([truth_vector[i] * log2(truth_vector[i]) for i in range(len(truth_vector))])\n",
    "print('entropy: %.3f bits' % entropy_target)\n",
    "\n",
    "# calculate cross entropy\n",
    "cross_entropy = entropy_target/entropy_data\n",
    "print('cross entropy: %.3f bits' % cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we use the base of 2. If the base of the logarithm is e, the entropy is measured in nats. Unless otherwise specified, we will take all logarithms to base 2, and hence all the entropies will be measured in bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5403397188137484\n",
      "The KL-divergence value between the data and the truth is 0.0017526649246738847 which is very close to 0\n",
      "Now consider the KL-divergence value between the data and the prediction: 0.8286530909355219 That's already a lot higher!\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "print(entropy(data_vector, base=2))\n",
    "# When using the entropy function from scipy we compute the KL-divergence between the two given vectors\n",
    "# If qk is not None, then compute the Kullback-Leibler divergence S = sum(pk * log(pk / qk), axis=axis).\n",
    "print(f\"The KL-divergence value between the data and the truth is {entropy(data_vector, truth_vector, base=2)} which is very close to 0\")\n",
    "print(f\"Now consider the KL-divergence value between the data and the prediction: {entropy(data_vector, pred_vector, base=2)} That's already a lot higher!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This all makes sense as we expect the first KL-divergence to be closer to 0 while the latter KL-divergence value is expected to have a significantly higher value compared to the first value. KL-divergence values are best used relatively or in a comparison. Concluding; two vectors are very similair (value close to 0) or definitely not (further away from 0). We note that the prediction vector is not as similair to the real data as the truth vector, we can thus conclude that our model is not making predictions according to the distribution of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy is a measure of information on a distribution. However, words in sequence can also be regarded as an outcome of a distribution. The language in this case determines the distribution. This means that you can use the probabilities found for certain words in the vocabulary to calculate the entropy of a sentence. If a language is thought of as a stochastic process that produces\n",
    " a sequence of words, its entropy rate is defined as (Jurasfky and Martin, 2009):\n",
    " \n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "    H(w_{1}w_{2}...w_{n}) & = -\\sum_{ w_{1}...w_{n}} {P}{w_{1}...w_{n}}\\log _{2}{P}(w_{n}\\vert w_{1}...w_{n-1}) \\\\\n",
    "    & =-\\sum_{ w_{1}...w_{n}} {P}{w_{1}...w_{n}}\\log _{2}{P}(w_{1}...w_{n}) \\\\ \n",
    "    & + \\sum_{w_{1}...w_{n-1}} {P}{w_{1}...w_{n-1}}\\log _{2}{P}(w_{1}...w_{n-1}) \n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "The assumption underlying entropy is an infinite possibilities of distributions. To measure the true entropy of a language (i.e. vocabulary) we need to consider sequences of infinite length. In other words, to get the true probabilities of words in sequences given the language contains many uncertainties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next notebook\n",
    "So far we have illustrated how text analysis evolved from linguistic rules, to counting frequencies to more advanced modeling approaches. The downside of all these approaches is that the ordering of words is most often ignored. An alternative method that do measure linguistic meaning captured in the ordering of words within a document or sentence are embeddings. This will be discussed in week 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
