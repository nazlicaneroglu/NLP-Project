# -*- coding: utf-8 -*-
"""
Created on Fri Jun 16 14:17:30 2023

@author: pc
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
import torch
import transformers as ppb
import warnings


df = pd.read_csv('new_dataset_for_ft.csv')
print(df.shape)
df.head()
batch_1 = df["text"][:1600]
batch_2 = df["text"][1600:]

model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')
tokenizer = tokenizer_class.from_pretrained(pretrained_weights)
model = model_class.from_pretrained(pretrained_weights)

tokenized = batch_1.apply(lambda x: tokenizer.encode(x, add_special_tokens=True))

max_len = 0
for i in tokenized.values:
    if len(i) > max_len:
        max_len = len(i)

padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])
#so we dont have enough memory available to allocate the required amount of memory for the tensors. 
#The error message specifically states that you tried to allocate 63137823744 bytes, which is approximately 63 GB.
#So we will use a batch of 2000 text
attention_mask = np.where(padded != 0, 1, 0)

input_ids = torch.tensor(padded)  
attention_mask = torch.tensor(attention_mask)

with torch.no_grad():
    last_hidden_states = model(input_ids, attention_mask=attention_mask)
